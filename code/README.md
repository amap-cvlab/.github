# Alibaba AMAP CV Lab

[ä¸­æ–‡é˜…è¯»](README_zh.md)

# ğŸ‘‹ About Us

The Alibaba AMAP CV Lab focuses on cutting-edge research and innovative applications centered around computer vision technology, dedicated to building the core technological capabilities of the spatiotemporal internet. 
Positioned at the intersection of the physical and digital worlds, we empower smart mobility, daily life, and virtual spaces through AI-driven understanding and generation.

As the core technical driving force behind AMAP, our research spans the entire chain from perception to generation, and from human-centric intelligence to world modeling. 
We are structured into six major research domains:
- ğŸ—ºï¸ Map & Autonomous Driving: Integrating multimodal perception with high-definition map generation to enable spatial semantic understanding and regulation-aware intelligent driving.
- ğŸ•ºğŸ» Human-Centric AI: Building AI systems that understand human emotion, identity, and behavior to achieve natural visual generation and interaction.
- ğŸ§­ Embodied Intelligence: Studying agents that perceive, plan, and act within both virtual and physical environments, unifying vision, language, and motion intelligence.
- ğŸŒ World Modeling: Constructing dynamic, interactive models of the world to empower AI with the ability to understand, predict, and generate complex environments.
- ğŸ§Š 3D Generation & Reconstruction: Advancing 3D scene modeling, rendering, and generation with continuous level-of-detail control and physically realistic synthesis.
- ğŸ§  General Deep Learning: Exploring general representation learning, model optimization, and multimodal alignment as foundational algorithms for spatiotemporal intelligence.

The AMAP CV Lab stands at the forefront of computer vision research and application, serving as a key technological practitioner in Alibabaâ€™s spatial intelligent internet.
We believe that AIâ€™s ability to understand the world defines the future of intelligent mobility and everyday life.

---

_We welcome contributions, issues, and feedback!_  
Feel free to â­ the repos below to stay updated.

# ğŸ”ˆ Latest News

- ğŸ› **Nov, 2025** â€“ We are thrilled to announce that **5 papers** from our team have been accepted to AAAI 2026, including 1 Oral, highlighting our latest breakthroughs in autonomous driving ([**UniMapGen**](https://arxiv.org/abs/2509.22262) (*Oral*), [**PriorDrive**](), [**Persistent Autoregressive Mapping**](https://arxiv.org/pdf/2509.22756)) and video generation & digital humans from our [**Fantasy AIGC Family**](https://github.com/Fantasy-AMAP) ([**FantasyTalking2**](https://fantasy-amap.github.io/fantasy-talking2/), [**FantasyHSI**](https://fantasy-amap.github.io/fantasy-hsi/)).
- ğŸ› **Sep, 2025** â€“ Our paper [**FutureSightDrive**](https://miv-xjtu.github.io/FSDrive.github.io/) is accepted by NeurIPS 2025 (Spotlight).
- ğŸ› **Jul, 2025** â€“ Our paper [**FantasyTalking**](https://fantasy-amap.github.io/fantasy-talking/) is accepted by ACM MM 2025.
- ğŸ› **Jun, 2025** â€“ Our paper [**SeqGrowGraph**](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_SeqGrowGraph_Learning_Lane_Topology_as_a_Chain_of_Graph_Expansions_ICCV_2025_paper.pdf) is accepted by ICCV 2025.
- ğŸ“¢ **May, 2025** â€“ We released the full project of [**FSDrive**](https://miv-xjtu.github.io/FSDrive.github.io/).
- ğŸ› **Apr, 2025** â€“ Our paper [**G3PT**](https://arxiv.org/abs/2409.06322) is accepted by IJCAI 2025.
- ğŸ“¢ **Apr, 2025** â€“ We released the inference code and model weights of [**FantasyTalking**](https://fantasy-amap.github.io/fantasy-talking/), [**FantasyID**](https://fantasy-amap.github.io/fantasy-id/).

# ğŸ”§ Public Technologies

<!-- Auto-generated Papers -->