[
    {
        "icon": "ğŸ—ºï¸",
        "title": "Map & Autonomous Driving",
        "title_zh": "åœ°å›¾ä¸è‡ªåŠ¨é©¾é©¶",
        "intro": "The core of our research lies in integrating perception, mapping, and decision-making for intelligent transportation. We develop next-generation 3D map engines, traffic rule reasoning, and scene-level behavior modeling, enabling AI to understand spatial context and make interpretable decisions in real-world urban environments.",
        "intro_zh": "èåˆæ„ŸçŸ¥ã€åœ°å›¾ä¸å†³ç­–çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ¨åŠ¨é«˜ç²¾åœ°å›¾ã€è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä¸æ—¶ç©ºæ™ºèƒ½çš„æ·±åº¦èåˆã€‚å›¢é˜Ÿèšç„¦äºæ„å»ºä¸‹ä¸€ä»£ 3D åœ°å›¾å¼•æ“ã€äº¤é€šè§„åˆ™ç†è§£ä¸åœºæ™¯çº§è¡Œä¸ºå»ºæ¨¡ï¼Œè®© AI åœ¨çœŸå®åŸå¸‚é“è·¯ä¸­å…·å¤‡ç©ºé—´ç†è§£ä¸å¯è§£é‡Šå†³ç­–èƒ½åŠ›ã€‚",
        "papers": [
            {
                "title": "ğŸš˜ FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
                "intro": "The first VLA for autonomous driving visual reasoning, which proposes spatio-temporal CoT to think visually about trajectory planning and unifies visual generation and understanding with minimal data.",
                "intro_zh": "åœ¨è‡ªåŠ¨é©¾é©¶æ–¹å‘é¦–æ¬¡æå‡ºä¸€ç§æ—¶ç©ºæ€ç»´é“¾çš„æ¨ç†æ–¹æ³•ï¼Œæå‡ºäº†è§†è§‰ç”Ÿæˆä¸ç†è§£ç»Ÿä¸€çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œå…è®¸æ¨¡å‹å¯è§†åŒ–åœ°æ€è€ƒï¼ŒåŸºäºå½“å‰è§‚å¯Ÿå’Œé¢„æµ‹çš„æœªæ¥ä¸–ç•Œè¿›è¡Œè½¨è¿¹è§„åˆ’ã€‚",
                "project": {
                    "id": "FSDrive",
                    "url": "https://miv-xjtu.github.io/FSDrive.github.io/"
                },
                "arxiv": {
                    "id": "2505.17685"
                },
                "publish": {
                    "id": "NeurIPS 2025 (Spotlight)",
                    "url": "https://neurips.cc/virtual/2025/poster/116"
                },
                "github": {
                    "repo": "MIV-XJTU/FSDrive",
                    "stars": true
                }
            },
            {
                "title": "ğŸ—º UniMapGen: A Generative Frameworkfor Large-Scale Map Construction from Multi-modal Data",
                "intro": "A generative unified framework that autoregressively generates smooth and topologically consistent vectorized maps from multi-modal inputs, enabling scalable, occlusion-robust city-scale mapping without costly on-site data collection.",
                "intro_zh": "ä¸€ä¸ªç”Ÿæˆå¼ç»Ÿä¸€æ¡†æ¶ï¼Œå¯ä»¥é€šè¿‡è‡ªå›å½’æ–¹å¼ä»å¤šæ¨¡æ€è¾“å…¥ä¸­ç”Ÿæˆå¹³æ»‘ä¸”æ‹“æ‰‘ä¸€è‡´çš„çŸ¢é‡åŒ–åœ°å›¾ï¼Œå®ç°å¯æ‰©å±•çš„ã€å¯¹é®æŒ¡å…·æœ‰é²æ£’æ€§çš„å¤§è§„æ¨¡åŸå¸‚åœ°å›¾æ„å»ºï¼ŒåŒæ—¶æ— éœ€æ˜‚è´µçš„ç°åœºæ•°æ®é‡‡é›†ã€‚",
                "project": {
                    "id": "UniMapGen",
                    "url": "https://amap-cvlab.github.io/UniMapGen/"
                },
                "arxiv": {
                    "id": "2509.22262"
                },
                "publish": {
                    "id": "AAAI 2026 (Oral)",
                    "url": "https://arxiv.org/pdf/2509.22262"
                },
                "github": {
                    "repo": "amap-cvlab/UniMapGen"
                }
            },
            {
                "title": "ğŸ›£ï¸ PriorDrive: Enhancing Online HD Mapping with Unified Vector Priors",
                "intro": "This is the first framework that unifies the encoding and integration of diverse vectorized prior maps (such as SD maps, outdated HD maps, and historical maps) to enhance online HD map construction.",
                "intro_zh": "é¦–ä¸ªèƒ½å¤Ÿç»Ÿä¸€ç¼–ç å¹¶èåˆå¤šç§çŸ¢é‡åŒ–å…ˆéªŒåœ°å›¾ï¼ˆå¦‚SDåœ°å›¾ã€è¿‡æœŸHDåœ°å›¾ã€å†å²åœ°å›¾ï¼‰ï¼Œä»¥å¢å¼ºåœ¨çº¿é«˜ç²¾åœ°å›¾æ„å»ºçš„æ¡†æ¶ã€‚",
                "project": {
                    "id": "PriorDrive",
                    "url": "https://miv-xjtu.github.io/PriorDrive/"
                },
                "arxiv": {
                    "id": "2409.05352"
                },
                "publish": {
                    "id": "AAAI 2026",
                    "url": "https://arxiv.org/abs/2409.05352"
                },
                "github": {
                    "repo": "MIV-XJTU/PriorDrive"
                }
            },
            {
                "title": "ğŸš¥ Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving",
                "intro": "Pioneering a generative co-reasoning paradigm in autonomous mapping, this work (PAMR) unifies the autoregressive construction of lane geometry and persistent traffic rules, enabling vehicles to build maps with long-term memory and consistent rule awareness across extended sequences.",
                "intro_zh": "æœ¬æ–‡æå‡ºä¸€ç§èåˆå‡ ä½•ä¸è§„åˆ™çš„ç”Ÿæˆå¼è”åˆæ¨ç†èŒƒå¼ï¼ˆPAMRï¼‰ï¼Œå®ƒä»¥è‡ªå›å½’æ–¹å¼ç»Ÿä¸€æ„å»ºè½¦é“å‘é‡ä¸æŒä¹…åŒ–äº¤é€šè§„åˆ™ï¼Œèµ‹äºˆè½¦è¾†è·¨è¶Šé•¿æ—¶åºçš„åœºæ™¯è®°å¿†ä¸è§„åˆ™éµå¾ªèƒ½åŠ›ã€‚",
                "project": {
                    "id": "PAMR",
                    "url": "https://miv-xjtu.github.io/PAMR/"
                },
                "arxiv": {
                    "id": "2509.22756"
                },
                "github": {
                    "repo": "MIV-XJTU/PAMR"
                }
            },
            {
                "title": "ğŸ“‘ SeqGrowGraph: Learning Lane Topology as a Chain of Graph Expansions",
                "intro": "A generative framework that reframes lane network learning as a process of incrementally building an adjacency matrix.",
                "intro_zh": "ä¸€ç§ä»¥å¢é‡å¼æ„å»ºé‚»æ¥çŸ©é˜µè¿‡ç¨‹é‡æ–°é˜é‡Šè½¦é“ç½‘å­¦ä¹ çš„ç”Ÿæˆæ¡†æ¶ã€‚",
                "arxiv": {
                    "id": "2507.04822v1"
                },
                "publish": {
                    "id": "ICCV 2025",
                    "url": "https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_SeqGrowGraph_Learning_Lane_Topology_as_a_Chain_of_Graph_Expansions_ICCV_2025_paper.pdf"
                }
            },
            {
                "title": "ğŸš— Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map",
                "intro": "Benchmark and multi-modal approach for integrating lane-level traffic sign regulations into vectorized HD maps.",
                "intro_zh": "äº¤é€šæ ‡å¿—ä¸­çš„è½¦é“çº§äº¤é€šè§„åˆ™ç†è§£ä¸ç»‘è·¯è¯„æµ‹åŸºå‡†åŠå¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆã€‚",
                "project": {
                    "id": "MapDR",
                    "url": "https://amap-cvlab.github.io/DriveByTheRules/"
                },
                "arxiv": {
                    "id": "2410.23780"
                },
                "publish": {
                    "id": "CVPR 2025 (Highlight)",
                    "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign_CVPR_2025_paper.pdf"
                }
            }
        ]
    },
    {
        "icon": "ğŸ•ºğŸ»",
        "title": "Human-Centric AI",
        "title_zh": "æ•°å­—äºº",
        "intro": "Centered on generative AI, our digital human research advances from driven generation to autonomous action. Through the [Fantasy AIGC Family](https://github.com/Fantasy-AMAP), we achieve expressive, identity-consistent, and physically realistic video generation via multimodal diffusion and 3D-aware modeling.",
        "intro_zh": "ä»¥ç”Ÿæˆå¼AIä¸ºæ ¸å¿ƒï¼Œæ¢ç´¢æ•°å­—äººä»â€œè¢«é©±åŠ¨â€åˆ°â€œè‡ªä¸»è¡ŒåŠ¨â€çš„è¿›åŒ–ã€‚å›¢é˜Ÿæå‡º [Fantasy AIGC ç³»åˆ—æ¨¡å‹](https://github.com/Fantasy-AMAP)ï¼Œè¦†ç›–è¡¨æƒ…é©±åŠ¨ã€è¯­éŸ³é©±åŠ¨ã€èº«ä»½ä¿æŒä¸åŠ¨ä½œç”Ÿæˆï¼Œå®ç°æƒ…æ„Ÿä¸°å¯Œã€èº«ä»½ä¸€è‡´ã€ç‰©ç†åˆç†çš„é«˜ä¿çœŸæ•°å­—äººè§†é¢‘ç”Ÿæˆã€‚",
        "papers": [
            {
                "title": "ğŸ—£ï¸ FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis",
                "intro": "The first Wan-based high-fidelity audio-driven avatar system that synchronizes facial expressions, lip motion, and body gestures in dynamic scenes through dual-stage audio-visual alignment and controllable motion modulation.",
                "intro_zh": "é¦–ä¸ªåŸºäº Wan çš„é«˜ä¿çœŸéŸ³é¢‘é©±åŠ¨è™šæ‹Ÿäººç³»ç»Ÿï¼Œé€šè¿‡åŒé˜¶æ®µéŸ³è§†å¯¹é½ä¸å¯æ§è¿åŠ¨è°ƒåˆ¶ï¼Œå®ç°åŠ¨æ€åœºæ™¯ä¸‹é¢éƒ¨è¡¨æƒ…ã€å”‡åŠ¨ä¸èº«ä½“å§¿æ€çš„ç²¾å‡†åŒæ­¥ã€‚",
                "project": {
                    "id": "FantasyTalking",
                    "url": "https://fantasy-amap.github.io/fantasy-talking/"
                },
                "arxiv": {
                    "id": "2504.04842"
                },
                "publish": {
                    "id": "ACM MM 2025",
                    "url": "https://dl.acm.org/doi/10.1145/3746027.3755217"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-talking",
                    "stars": true
                },
                "huggingface": {
                    "repo": "acvlab/FantasyTalking",
                    "model": true,
                    "space": true
                },
                "modelscope": {
                    "repo": "amap_cvlab/FantasyTalking",
                    "model": true
                }
            },
            {
                "title": "ğŸ™ï¸ FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation",
                "intro": "A novel Timestep-Layer Adaptive Multi-Expert Preference Optimization (TLPO) method enhances the quality of audio-driven avatar in three dimensions: lip-sync, motion naturalness, and visual quality.",
                "intro_zh": "ä¸€ç§æ–°é¢–çš„â€œæ—¶é—´æ­¥-ç½‘ç»œå±‚â€è‡ªé€‚åº”å¤šä¸“å®¶åå¥½ä¼˜åŒ–(TLPO)æ–¹æ³•ï¼Œåœ¨å£å‹ä¸€è‡´ã€åŠ¨ä½œè‡ªç„¶ã€è§†è§‰æ•ˆæœä¸‰ä¸ªç»´åº¦ä¸Šæå‡äº†éŸ³é¢‘é©±åŠ¨æ•°å­—äººåŠ¨ç”»çš„è´¨é‡ã€‚",
                "project": {
                    "id": "FantasyTalking2",
                    "url": "https://fantasy-amap.github.io/fantasy-talking2/"
                },
                "arxiv": {
                    "id": "2508.11255v1"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-talking2",
                    "not_finished": true
                },
                "publish": {
                    "id": "AAAI 2026",
                    "url": "https://doi.org/10.48550/arXiv.2508.11255"
                }
            },
            {
                "title": "ğŸ—¿ FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework",
                "intro": "A graph-based multi-agent framework that grounds video generation within 3D world dynamics, enabling digital humans to perceive, plan, and act autonomously, thus serving as the technical bridge that links human modeling to world modeling through unified perceptionâ€“action reasoning.",
                "intro_zh": "ä¸€ç§åŸºäºå›¾ç»“æ„çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†è§†é¢‘ç”Ÿæˆä¸ä¸‰ç»´ä¸–ç•ŒåŠ¨æ€ç›¸èåˆï¼Œä½¿æ•°å­—äººå…·å¤‡æ„ŸçŸ¥ã€è§„åˆ’ä¸è‡ªä¸»è¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œä»è€Œåœ¨æŠ€æœ¯å±‚é¢ä¸Šæˆä¸ºè¿æ¥äººä¸ä¸–ç•Œçš„ç»Ÿä¸€â€œæ„ŸçŸ¥â€“è¡ŒåŠ¨â€æ¨ç†æ¡¥æ¢ã€‚",
                "project": {
                    "id": "FantasyHSI",
                    "url": "https://fantasy-amap.github.io/fantasy-hsi/"
                },
                "arxiv": {
                    "id": "2509.01232"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-hsi",
                    "not_finished": true
                },
                "publish": {
                    "id": "AAAI 2026",
                    "url": "https://doi.org/10.48550/arXiv.2509.01232"
                }
            },
            {
                "title": "ğŸ¤¡ FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers",
                "intro": "A novel expression-driven video-generation method that pairs emotion-enhanced learning with masked cross-attention, enabling the creation of high-quality, richly expressive animations for both single and multi-portrait scenarios.",
                "intro_zh": "ä¸€ç§å…¨æ–°çš„è¡¨æƒ…é©±åŠ¨è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œå°†æƒ…ç»ªå¢å¼ºå­¦ä¹ ä¸æ©ç äº¤å‰æ³¨æ„åŠ›ç›¸ç»“åˆï¼Œå¯åœ¨å•äººæˆ–å¤šäººè‚–åƒåœºæ™¯ä¸­ç”Ÿæˆé«˜è´¨é‡ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„åŠ¨ç”»ã€‚",
                "project": {
                    "id": "FantasyPortrait",
                    "url": "https://fantasy-amap.github.io/fantasy-portrait/"
                },
                "arxiv": {
                    "id": "2507.12956"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-portrait",
                    "stars": true
                }
            },
            {
                "title": "ğŸ†” FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation",
                "intro": "A tuning-free text-to-video model that leverages 3D facial priors, multi-view augmentation, and layer-aware guidance injection to deliver dynamic, identity-preserving video generation.",
                "intro_zh": "ä»¥3Dé¢éƒ¨å…ˆéªŒã€å¤šè§†è§’å¢å¼ºä»¥åŠå±‚æ„ŸçŸ¥æ³¨å…¥çš„æå‡è¿åŠ¨åœºæ™¯ä¸‹çš„IDä¿æŒè§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚",
                "project": {
                    "id": "FantasyID",
                    "url": "https://fantasy-amap.github.io/fantasy-id/"
                },
                "arxiv": {
                    "id": "2502.13995"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-id"
                },
                "huggingface": {
                    "repo": "acvlab/FantasyID",
                    "model": true
                },
                "modelscope": {
                    "repo": "amap_cvlab/FantasyID",
                    "model": true
                }
            },
            {
                "title": "ğŸ’ƒğŸ» HumanRig: Learning Automatic Rigging for Humanoid Characters in Animation",
                "intro": "The first dataset for automatic rigging of 3D generated digital humans and a transformer-based end-to-end automatic rigging algorithm.",
                "intro_zh": "é¦–ä¸ªé¢å‘3Dç”Ÿæˆæ•°å­—äººçš„è‡ªåŠ¨ç»‘éª¨æ•°æ®é›†ä»¥åŠåŸºäºå˜æ¢å™¨çš„ç«¯åˆ°ç«¯è‡ªåŠ¨ç»‘éª¨ç®—æ³•ã€‚",
                "project": {
                    "id": "HumanRig",
                    "url": "https://c8241998.github.io/HumanRig/"
                },
                "arxiv": {
                    "id": "2412.02317"
                },
                "publish": {
                    "id": "CVPR 2025 (Highlight)",
                    "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large_CVPR_2025_paper.pdf"
                },
                "github": {
                    "repo": "c8241998/HumanRig"
                },
                "huggingface": {
                    "repo": "jellyczd/HumanRig",
                    "dataset": true
                }
            }
        ]
    },
    {
        "icon": "ğŸ§­",
        "title": "Embodied AI",
        "title_zh": "å…·èº«æ™ºèƒ½",
        "intro": "We study perception, reasoning, and action of intelligent agents in both virtual and physical environments. By integrating vision-language models and reinforcement learning, we build embodied agents capable of environmental perception, goal planning, and task execution, forming a unified cognitive foundation for robots and digital humans.",
        "intro_zh": "ç ”ç©¶æ™ºèƒ½ä½“åœ¨è™šæ‹Ÿä¸ç‰©ç†ç¯å¢ƒä¸­çš„æ„ŸçŸ¥ã€æ€è€ƒä¸è¡ŒåŠ¨æœºåˆ¶ã€‚é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ çš„ç»“åˆï¼Œæ„å»ºå¯åœ¨ä¸‰ç»´ç©ºé—´ä¸­æ„ŸçŸ¥ç¯å¢ƒã€è§„åˆ’ç›®æ ‡ã€æ‰§è¡Œä»»åŠ¡çš„å…·èº«æ™ºèƒ½ä½“ï¼Œä¸ºæœºå™¨äººä¸è™šæ‹Ÿäººæä¾›ç»Ÿä¸€çš„è®¤çŸ¥æ¡†æ¶ã€‚",
        "papers": [
            {
                "title": "CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation",
                "intro": "A novel cross-embodiment local navigation framework, which can serve as a \"one brain, multiple forms\", plug-and-play fast system.",
                "intro_zh": "ä¸€ä¸ªæ–°é¢–çš„è·¨å…·èº«å®ä½“çš„å±€éƒ¨å¯¼èˆªæ¡†æ¶ï¼Œå¯ç”¨ä½œä¸€è„‘å¤šå½¢ã€å¯æ’æ‹”çš„å¿«ç³»ç»Ÿã€‚",
                "project": {
                    "id": "CE-Nav",
                    "url": "https://ce-nav.github.io/"
                },
                "arxiv": {
                    "id": "2509.23203"
                },
                "github": {
                    "repo": "amap-cvlab/CE-Nav"
                }
            },
            {
                "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation",
                "intro": "OmniNav is a unified embodied navigation framework that combines a lightweight, real-time (up to 5 Hz) continuous waypoint policy with a fastâ€“slow planning architecture and large-scale vision-language multi-task training to robustly handle instruction-, object-, and point-goal navigation and frontier exploration, achieving state-of-the-art performance and real-world validation.",
                "intro_zh": "OmniNavæå‡ºç»Ÿä¸€çš„æœºå™¨äººå¯¼èˆªæ¡†æ¶ï¼Œä»¥ä½å»¶è¿Ÿçš„è¿ç»­èˆªç‚¹ç­–ç•¥ä¸å¿«æ…¢ååŒè§„åˆ’ç»“åˆå¤šä»»åŠ¡ã€é€šç”¨è§†è§‰è¯­è¨€æ•°æ®å¢å¼ºç†è§£èƒ½åŠ›ï¼Œåœ¨æŒ‡ä»¤ç›®æ ‡ã€ç‰©ä½“ç›®æ ‡ã€ç‚¹ç›®æ ‡åŠå‰æ²¿æ¢ç´¢ä»»åŠ¡ä¸Šå®ç°æ›´é«˜ç²¾åº¦ã€æ³›åŒ–ä¸æˆåŠŸç‡ï¼Œå¹¶è·çœŸå®éƒ¨ç½²éªŒè¯ã€‚",
                "arxiv": {
                    "id": "2509.25687"
                },
                "github": {
                    "repo": "amap-cvlab/OmniNav",
                    "not_finished": true
                }
            },
            {
                "title": "ğŸ§  JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
                "intro": "The first visual-language navigation agent with dual implicit memory decouples visual semantics and spatial perception and models them respectively as compact implicit neural representations.",
                "intro_zh": "é¦–ä¸ªå…·å¤‡åŒé‡éšå¼è®°å¿†çš„è§†è§‰è¯­è¨€å¯¼èˆªæ™ºèƒ½ä½“ï¼Œè§£è€¦è§†è§‰è¯­ä¹‰å’Œç©ºé—´æ„ŸçŸ¥ï¼Œå¹¶åˆ†åˆ«å»ºæ¨¡ä¸ºç´§å‡‘çš„éšå¼ç¥ç»è¡¨ç¤ºã€‚",
                "project": {
                    "id": "JanusVLN",
                    "url": "https://miv-xjtu.github.io/JanusVLN.github.io/"
                },
                "arxiv": {
                    "id": "2509.22548"
                },
                "github": {
                    "repo": "MIV-XJTU/JanusVLN",
                    "stars": true
                },
                "modelscope": {
                    "repo": "misstl/JanusVLN_Extra",
                    "model": true
                }
            },
            {
                "title": "Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA",
                "intro": "A Robust Vision-Language-Action Framework with Structural Perception and Explicit Dynamics Reasoning.",
                "intro_zh": "èåˆç©ºé—´ç»“æ„ä¸åŠ¨æ€æ¨ç†çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ–°èŒƒå¼ã€‚",
                "arxiv": {
                    "id": "2509.26251"
                }
            }
        ]
    },
    {
        "icon": "ğŸŒ",
        "title": "World Modeling",
        "title_zh": "ä¸–ç•Œæ¨¡å‹",
        "intro": "We aim to construct dynamic, interactive world models for understanding, predicting, and generating physically consistent spatiotemporal phenomena. By leveraging multimodal modeling and generative learning, our research enables a perception-to-simulation loop that empowers AI to comprehend and recreate the real world.",
        "intro_zh": "è‡´åŠ›äºæ„å»ºåŠ¨æ€ã€å¯äº¤äº’çš„ä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºç†è§£ã€é¢„æµ‹ä¸ç”Ÿæˆç‰©ç†ä¸€è‡´çš„æ—¶ç©ºè¿‡ç¨‹ã€‚é€šè¿‡è·¨æ¨¡æ€æ•°æ®å»ºæ¨¡ä¸ç”Ÿæˆå¼å­¦ä¹ ï¼Œå®ç°ä»æ„ŸçŸ¥åˆ°æ¨¡æ‹Ÿçš„é—­ç¯ï¼Œè®©AIå…·å¤‡ç†è§£çœŸå®ä¸–ç•Œçš„èƒ½åŠ›ã€‚",
        "papers": [
            {
                "title": "ğŸŒ FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction",
                "intro": "A unified world model integrating video priors and geometric grounding for synthesizing explorable and geometrically consistent 3D scenes.",
                "intro_zh": "ä¸€ä¸ªç»Ÿä¸€è§†é¢‘å…ˆéªŒä¿¡æ¯å’Œå‡ ä½•3Dçš„ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡ ä½•ä¸€è‡´çš„ã€å¯æ¢ç´¢çš„3Dåœºæ™¯ã€‚",
                "arxiv": {
                    "id": "2509.21657"
                },
                "project": {
                    "id": "FantasyWorld",
                    "url": "https://fantasy-amap.github.io/fantasy-world/"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-world",
                    "not_finished": true
                }
            },
            {
                "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training",
                "intro": "A novel framework leveraging world model as a virtual environment for VLA post training.",
                "intro_zh": "ä¸€ä¸ªæ–°é¢–çš„ä»¥ä¸–ç•Œæ¨¡å‹ä¸ºè™šæ‹Ÿç¯å¢ƒçš„VLAåè®­ç»ƒæ¡†æ¶ã€‚",
                "arxiv": {
                    "id": "2509.24948"
                }
            }
        ]
    },
    {
        "icon": "ğŸ§Š",
        "title": "3D Generation & Reconstruction",
        "title_zh": "3Dç”Ÿæˆä¸é‡å»º",
        "intro": "Our research in 3D generation and reconstruction covers Gaussian Splatting, NeRF, and 3D-aware diffusion, aiming for real-time rendering, continuous level-of-detail control, and semantically consistent 3D scene synthesis.",
        "intro_zh": "æ¢ç´¢3Dä¸–ç•Œçš„ç”Ÿæˆå¼å»ºæ¨¡ä¸é«˜ä¿çœŸé‡å»ºã€‚ç ”ç©¶æ–¹å‘æ¶µç›– Gaussian Splattingã€NeRFã€3D-aware diffusion ç­‰æŠ€æœ¯ï¼Œç”¨äºå®ç°å®æ—¶æ¸²æŸ“ã€è¿ç»­ç»†èŠ‚å±‚æ¬¡ï¼ˆLODï¼‰æ§åˆ¶ä¸è¯­ä¹‰ä¸€è‡´çš„ä¸‰ç»´åœºæ™¯ç”Ÿæˆã€‚",
        "papers": [
            {
                "title": "ğŸ§¸ G3PT: Unleash the Power of Autoregressive Modeling in 3D Generative Tasks",
                "intro": "The first native 3D generation foundational model based on next-scale autoregression.",
                "intro_zh": "é¦–ä¸ªåŸºäºå¤šå°ºåº¦è‡ªå›å½’çš„åŸç”Ÿ 3D ç”ŸæˆåŸºåº§å¤§æ¨¡å‹ã€‚",
                "arxiv": {
                    "id": "2409.06322"
                },
                "publish": {
                    "id": "IJCAI 2025",
                    "url": "https://www.ijcai.org/proceedings/2025/262"
                }
            },
            {
                "title": "ğŸ™ Global-Guided Focal Neural Radiance Field for Large-Scale Scene Representation",
                "intro": "GF-NeRF introduces a global-guided two-stage architecture to achieve consistent and high-fidelity large-scale scene rendering without relying on prior scene knowledge.",
                "intro_zh": "GF-NeRF é€šè¿‡å…¨å±€å¼•å¯¼çš„åŒé˜¶æ®µæ¶æ„ï¼Œå®ç°æ— éœ€å…ˆéªŒçŸ¥è¯†çš„å¤§è§„æ¨¡åœºæ™¯ä¸€è‡´ä¸”é«˜ä¿çœŸæ¸²æŸ“ã€‚",
                "project": {
                    "id": "GF-NeRF",
                    "url": "https://shaomq2187.github.io/GF-NeRF/"
                },
                "arxiv": {
                    "id": "2403.12839"
                },
                "publish": {
                    "id": "WACV 2025",
                    "url": "https://ieeexplore.ieee.org/abstract/document/10943871"
                }
            },
            {
                "title": "ğŸ’  CLoD-GS: Continuous Level-of-Detail Gaussian Splatting for Real-Time Rendering",
                "intro": "CLoD-GS equips 3D Gaussian Splatting with learnable distance-adaptive opacity, enabling smooth, storage-efficient, artifact-free continuous level-of-detail rendering from a single model.",
                "intro_zh": "CLoD-GS é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è·ç¦»è‡ªé€‚åº”é€æ˜åº¦ï¼Œä¸º 3D é«˜æ–¯å–·æº…è¡¨ç¤ºå®ç°å•ä¸€æ¨¡å‹å†…å¹³æ»‘ã€æ— å­˜å‚¨å†—ä½™ã€æ— è·³å˜ä¼ªå½±çš„è¿ç»­ç»†èŠ‚å±‚æ¬¡æ¸²æŸ“ã€‚",
                "arxiv": {
                    "id": "2510.09997"
                },
                "github": {
                    "repo": "amap-cvlab/CLoD-GS"
                }
            }
        ]
    },
    {
        "icon": "ğŸ§ ",
        "title": "General Deep Learning",
        "title_zh": "é€šç”¨æ·±åº¦å­¦ä¹ ",
        "intro": "We focus on general representation learning and model optimization as the foundation for multimodal and cross-domain AI systems. Our research includes Transformer architecture optimization, distributed training, model compression, and preference alignment (DPO, RLHF) to enhance generalization and interpretability.",
        "intro_zh": "å…³æ³¨é€šç”¨è¡¨ç¤ºå­¦ä¹ ä¸æ¨¡å‹ä¼˜åŒ–ï¼Œä¸ºå¤šæ¨¡æ€ã€è·¨ä»»åŠ¡AIç³»ç»Ÿæä¾›ç»Ÿä¸€åŸºç¡€ã€‚ç ”ç©¶æ–¹å‘åŒ…æ‹¬ Transformeræ¶æ„ä¼˜åŒ–ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ¨¡å‹å‹ç¼© ä¸ åå¥½å¯¹é½å­¦ä¹ ï¼ˆDPO, RLHFï¼‰ï¼ŒæŒç»­æå‡æ¨¡å‹çš„æ³›åŒ–æ€§ä¸å¯è§£é‡Šæ€§ã€‚",
        "papers": [
            {
                "title": "ğŸ™ï¸ A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
                "intro": "Performance analysis and novel solution exploration for speech recognition under synthetic speech interference.",
                "intro_zh": "åˆæˆè¯­éŸ³å¹²æ‰°ä¸‹ï¼Œè¯­éŸ³è¯†åˆ«æ€§èƒ½åˆ†æå’Œæ–°æ–¹æ¡ˆæ¢ç´¢ã€‚",
                "publish": {
                    "id": "ICASSP 2024",
                    "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10446991"
                }
            },
            {
                "title": "Doubly-Fused ViT: Fuse Information from Dual Vision Transformer Streams",
                "intro": "DFvT introduces a doubly-fused Vision Transformer that combines efficient global context modeling with fine-grained spatial detail preservation to achieve high accuracy and efficiency.",
                "intro_zh": "DFvT æå‡ºä¸€ç§åŒèåˆè§†è§‰Transformeræ¶æ„ï¼Œå…¼é¡¾å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸ç²¾ç»†ç©ºé—´ç»†èŠ‚ä¿ç•™ï¼Œåœ¨ä¿è¯é«˜æ•ˆç‡çš„åŒæ—¶å®ç°é«˜ç²¾åº¦è¡¨ç°ã€‚",
                "publish": {
                    "id": "ECCV 2022",
                    "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830723.pdf"
                },
                "github": {
                    "repo": "ginobilinie/DFvT"
                }
            },
            {
                "title": "SCMT: Self-Correction Mean Teacher for Semi-supervised Object Detection",
                "intro": "A self-correction mean teacher architecture that mitigates the impact of noisy pseudo-labels, offering a novel technological breakthrough in the field of semi-supervised object detection.",
                "intro_zh": "ä¸€ç§é€šè¿‡è‡ªæˆ‘æ ¡æ­£çš„æ•™å¸ˆæ¶æ„æ¥å‡å°‘å™ªå£°ä¼ªæ ‡ç­¾å½±å“çš„åŠç›‘ç£ç›®æ ‡æ£€æµ‹æ–°æ–¹æ³•ã€‚",
                "publish": {
                    "id": "IJCAI 2022",
                    "url": "https://www.ijcai.org/proceedings/2022/0207.pdf"
                }
            },
            {
                "title": "DPOSE: Online Keypoint-CAM Guided Inference for Driver Pose Estimation",
                "intro": "An optimization scheme for a proprietary HPE task in DMS scenarios which involves a pose-wise hard mining strategy for distribution balance and an online keypoint-aligned Grad-CAM loss to constrain activations to semantic regions.",
                "intro_zh": "é’ˆå¯¹DMSåœºæ™¯ä¸‹çš„HPEä»»åŠ¡ï¼Œæå‡ºåŒ…å«å›°éš¾æ ·æœ¬æŒ–æ˜ä¸åœ¨çº¿å…³é”®ç‚¹å¯¹é½Grad-CAMæŸå¤±çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚",
                "publish": {
                    "id": "CVPR Workshop 2023",
                    "url": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Guo_DPOSE_Online_Keypoint-CAM_Guided_Inference_for_Driver_Pose_Estimation_With_CVPRW_2023_paper.pdf"
                }
            }
        ]
    },
    {
        "icon": "",
        "title": "",
        "title_zh": "",
        "intro": "",
        "intro_zh": "",
        "papers": [
            {
                "title": "",
                "intro": "",
                "intro_zh": "",
                "project": {
                    "id": "",
                    "url": ""
                },
                "arxiv": {
                    "id": ""
                },
                "publish": {
                    "id": "",
                    "url": ""
                },
                "github": {
                    "repo": ""
                },
                "hf_model": {
                    "id": ""
                },
                "hf_space": {
                    "id": ""
                },
                "hf_dataset": {
                    "id": ""
                }
            }
        ]
    }
]