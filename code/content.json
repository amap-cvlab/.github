[
    {
        "icon": "🗺️",
        "title": "Map & Autonomous Driving",
        "title_zh": "地图与自动驾驶",
        "intro": "The core of our research lies in integrating perception, mapping, and decision-making for intelligent transportation. We develop next-generation 3D map engines, traffic rule reasoning, and scene-level behavior modeling, enabling AI to understand spatial context and make interpretable decisions in real-world urban environments.",
        "intro_zh": "融合感知、地图与决策的核心技术，推动高精地图、自动驾驶感知与时空智能的深度融合。团队聚焦于构建下一代 3D 地图引擎、交通规则理解与场景级行为建模，让 AI 在真实城市道路中具备空间理解与可解释决策能力。",
        "papers": [
            {
                "title": "🚘 FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
                "intro": "The first VLA for autonomous driving visual reasoning, which proposes spatio-temporal CoT to think visually about trajectory planning and unifies visual generation and understanding with minimal data.",
                "intro_zh": "在自动驾驶方向首次提出一种时空思维链的推理方法，提出了视觉生成与理解统一的预训练范式，允许模型可视化地思考，基于当前观察和预测的未来世界进行轨迹规划。",
                "project": {
                    "id": "FSDrive",
                    "url": "https://miv-xjtu.github.io/FSDrive.github.io/"
                },
                "arxiv": {
                    "id": "2505.17685"
                },
                "publish": {
                    "id": "NeurIPS 2025 (Spotlight)",
                    "url": "https://neurips.cc/virtual/2025/poster/116"
                },
                "github": {
                    "repo": "MIV-XJTU/FSDrive"
                }
            },
            {
                "title": "📑 SeqGrowGraph: Learning Lane Topology as a Chain of Graph Expansions",
                "intro": "A generative framework that reframes lane network learning as a process of incrementally building an adjacency matrix.",
                "intro_zh": "一种以增量式构建邻接矩阵过程重新阐释车道网学习的生成框架。",
                "arxiv": {
                    "id": "2507.04822v1"
                },
                "publish": {
                    "id": "ICCV 2025",
                    "url": "https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_SeqGrowGraph_Learning_Lane_Topology_as_a_Chain_of_Graph_Expansions_ICCV_2025_paper.pdf"
                }
            },
            {
                "title": "🚗 Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map",
                "intro": "Benchmark and multi-modal approach for integrating lane-level traffic sign regulations into vectorized HD maps.",
                "intro_zh": "交通标志中的车道级交通规则理解与绑路评测基准及多模态解决方案。",
                "project": {
                    "id": "MapDR",
                    "url": "https://amap-cvlab.github.io/DriveByTheRules/"
                },
                "arxiv": {
                    "id": "2410.23780"
                },
                "publish": {
                    "id": "CVPR 2025 (Highlight)",
                    "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign_CVPR_2025_paper.pdf"
                }
            }
        ]
    },
    {
        "icon": "🕺🏻",
        "title": "Human-Centric AI",
        "title_zh": "数字人",
        "intro": "Centered on generative AI, our digital human research advances from driven generation to autonomous action. Through the [Fantasy AIGC Family](https://github.com/Fantasy-AMAP), we achieve expressive, identity-consistent, and physically realistic video generation via multimodal diffusion and 3D-aware modeling.",
        "intro_zh": "以生成式AI为核心，探索数字人从“被驱动”到“自主行动”的进化。团队提出 [Fantasy AIGC 系列模型](https://github.com/Fantasy-AMAP)，覆盖表情驱动、语音驱动、身份保持与动作生成，实现情感丰富、身份一致、物理合理的高保真数字人视频生成。",
        "papers": [
            {
                "title": "🗣️ FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis",
                "intro": "The first Wan-based high-fidelity audio-driven avatar system that synchronizes facial expressions, lip motion, and body gestures in dynamic scenes through dual-stage audio-visual alignment and controllable motion modulation.",
                "intro_zh": "首个基于 Wan 的高保真音频驱动虚拟人系统，通过双阶段音视对齐与可控运动调制，实现动态场景下面部表情、唇动与身体姿态的精准同步。",
                "project": {
                    "id": "FantasyTalking",
                    "url": "https://fantasy-amap.github.io/fantasy-talking/"
                },
                "arxiv": {
                    "id": "2504.04842"
                },
                "publish": {
                    "id": "ACM MM 2025",
                    "url": "https://dl.acm.org/doi/10.1145/3746027.3755217"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-talking",
                    "stars": true
                },
                "huggingface": {
                    "repo": "acvlab/FantasyTalking",
                    "model": true,
                    "space": true
                },
                "modelscope": {
                    "repo": "amap_cvlab/FantasyTalking",
                    "model": true
                }
            },
            {
                "title": "🤡 FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers",
                "intro": "A novel expression-driven video-generation method that pairs emotion-enhanced learning with masked cross-attention, enabling the creation of high-quality, richly expressive animations for both single and multi-portrait scenarios.",
                "intro_zh": "一种全新的表情驱动视频生成方法，将情绪增强学习与掩码交叉注意力相结合，可在单人或多人肖像场景中生成高质量且富有表现力的动画。",
                "project": {
                    "id": "FantasyPortrait",
                    "url": "https://fantasy-amap.github.io/fantasy-portrait/"
                },
                "arxiv": {
                    "id": "2507.12956"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-portrait",
                    "stars": true
                }
            },
            {
                "title": "🗣️ FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation",
                "intro": "A novel Timestep-Layer Adaptive Multi-Expert Preference Optimization (TLPO) method enhances the quality of audio-driven avatar in three dimensions: lip-sync, motion naturalness, and visual quality.",
                "intro_zh": "一种新颖的“时间步-网络层”自适应多专家偏好优化(TLPO)方法，在口型一致、动作自然、视觉效果三个维度上提升了音频驱动数字人动画的质量。",
                "project": {
                    "id": "FantasyTalking2",
                    "url": "https://fantasy-amap.github.io/fantasy-talking2/"
                },
                "arxiv": {
                    "id": "2508.11255v1"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-talking2"
                }
            },
            {
                "title": "🆔 FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation",
                "intro": "A tuning-free text-to-video model that leverages 3D facial priors, multi-view augmentation, and layer-aware guidance injection to deliver dynamic, identity-preserving video generation.",
                "intro_zh": "以3D面部先验、多视角增强以及层感知注入的提升运动场景下的ID保持视频生成框架。",
                "project": {
                    "id": "FantasyID",
                    "url": "https://fantasy-amap.github.io/fantasy-id/"
                },
                "arxiv": {
                    "id": "2502.13995"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-id"
                },
                "huggingface": {
                    "repo": "acvlab/FantasyID",
                    "model": true
                },
                "modelscope": {
                    "repo": "amap_cvlab/FantasyID",
                    "model": true
                }
            },
            {
                "title": "🗿 FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework",
                "intro": "A graph-based multi-agent framework that grounds video generation within 3D world dynamics, enabling digital humans to perceive, plan, and act autonomously, thus serving as the technical bridge that links human modeling to world modeling through unified perception–action reasoning.",
                "intro_zh": "一种基于图结构的多智能体框架，将视频生成与三维世界动态相融合，使数字人具备感知、规划与自主行动的能力，从而在技术层面上成为连接人与世界的统一“感知–行动”推理桥梁。",
                "project": {
                    "id": "FantasyHSI",
                    "url": "https://fantasy-amap.github.io/fantasy-hsi/"
                },
                "arxiv": {
                    "id": "2509.01232"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-hsi",
                    "not_finished": true
                }
            },
            {
                "title": "💃🏻 HumanRig: Learning Automatic Rigging for Humanoid Characters in Animation",
                "intro": "The first dataset for automatic rigging of 3D generated digital humans and a transformer-based end-to-end automatic rigging algorithm.",
                "intro_zh": "首个面向3D生成数字人的自动绑骨数据集以及基于变换器的端到端自动绑骨算法。",
                "project": {
                    "id": "HumanRig",
                    "url": "https://c8241998.github.io/HumanRig/"
                },
                "arxiv": {
                    "id": "2412.02317"
                },
                "publish": {
                    "id": "CVPR 2025 (Highlight)",
                    "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large_CVPR_2025_paper.pdf"
                },
                "github": {
                    "repo": "c8241998/HumanRig"
                },
                "huggingface": {
                    "repo": "jellyczd/HumanRig",
                    "dataset": true
                }
            }
        ]
    },
    {
        "icon": "🧭",
        "title": "Embodied AI",
        "title_zh": "具身智能",
        "intro": "We study perception, reasoning, and action of intelligent agents in both virtual and physical environments. By integrating vision-language models and reinforcement learning, we build embodied agents capable of environmental perception, goal planning, and task execution, forming a unified cognitive foundation for robots and digital humans.",
        "intro_zh": "研究智能体在虚拟与物理环境中的感知、思考与行动机制。通过视觉语言模型与强化学习的结合，构建可在三维空间中感知环境、规划目标、执行任务的具身智能体，为机器人与虚拟人提供统一的认知框架。",
        "papers": [
            {
                "title": "CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation",
                "intro": "A novel cross-embodiment local navigation framework, which can serve as a \"one brain, multiple forms\", plug-and-play fast system.",
                "intro_zh": "一个新颖的跨具身实体的局部导航框架，可用作一脑多形、可插拔的快系统。",
                "project": {
                    "id": "CE-Nav",
                    "url": "https://ce-nav.github.io/"
                },
                "arxiv": {
                    "id": "2509.23203"
                },
                "github": {
                    "repo": "amap-cvlab/CE-Nav"
                }
            },
            {
                "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation",
                "intro": "OmniNav is a unified embodied navigation framework that combines a lightweight, real-time (up to 5 Hz) continuous waypoint policy with a fast–slow planning architecture and large-scale vision-language multi-task training to robustly handle instruction-, object-, and point-goal navigation and frontier exploration, achieving state-of-the-art performance and real-world validation.",
                "intro_zh": "OmniNav提出统一的机器人导航框架，以低延迟的连续航点策略与快慢协同规划结合多任务、通用视觉语言数据增强理解能力，在指令目标、物体目标、点目标及前沿探索任务上实现更高精度、泛化与成功率，并获真实部署验证。",
                "arxiv": {
                    "id": "2509.25687"
                },
                "github": {
                    "repo": "amap-cvlab/OmniNav"
                }
            },
            {
                "title": "🧠 JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
                "intro": "The first visual-language navigation agent with dual implicit memory decouples visual semantics and spatial perception and models them respectively as compact implicit neural representations.",
                "intro_zh": "首个具备双重隐式记忆的视觉语言导航智能体，解耦视觉语义和空间感知，并分别建模为紧凑的隐式神经表示。",
                "project": {
                    "id": "JanusVLN",
                    "url": "https://miv-xjtu.github.io/JanusVLN.github.io/"
                },
                "arxiv": {
                    "id": "2509.22548"
                },
                "github": {
                    "repo": "MIV-XJTU/JanusVLN",
                    "stars": true
                },
                "modelscope": {
                    "repo": "misstl/JanusVLN_Extra",
                    "model": true
                }
            },
            {
                "title": "Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA",
                "intro": "A Robust Vision-Language-Action Framework with Structural Perception and Explicit Dynamics Reasoning.",
                "intro_zh": "融合空间结构与动态推理的视觉-语言-动作新范式。",
                "arxiv": {
                    "id": "2509.26251"
                }
            }
        ]
    },
    {
        "icon": "🌐",
        "title": "World Modeling",
        "title_zh": "世界模型",
        "intro": "We aim to construct dynamic, interactive world models for understanding, predicting, and generating physically consistent spatiotemporal phenomena. By leveraging multimodal modeling and generative learning, our research enables a perception-to-simulation loop that empowers AI to comprehend and recreate the real world.",
        "intro_zh": "致力于构建动态、可交互的世界模型，用于理解、预测与生成物理一致的时空过程。通过跨模态数据建模与生成式学习，实现从感知到模拟的闭环，让AI具备理解真实世界的能力。",
        "papers": [
            {
                "title": "🌏 FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction",
                "intro": "A unified world model integrating video priors and geometric grounding for synthesizing explorable and geometrically consistent 3D scenes.",
                "intro_zh": "一个统一视频先验信息和几何3D的世界模型，能够生成几何一致的、可探索的3D场景。",
                "arxiv": {
                    "id": "2509.21657"
                },
                "project": {
                    "id": "FantasyWorld",
                    "url": "https://fantasy-amap.github.io/fantasy-world/"
                },
                "github": {
                    "repo": "Fantasy-AMAP/fantasy-world",
                    "not_finished": true
                }
            },
            {
                "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training",
                "intro": "A novel framework leveraging world model as a virtual environment for VLA post training.",
                "intro_zh": "一个新颖的以世界模型为虚拟环境的VLA后训练框架。",
                "arxiv": {
                    "id": "2509.24948"
                }
            }
        ]
    },
    {
        "icon": "🧊",
        "title": "3D Generation & Reconstruction",
        "title_zh": "3D生成与重建",
        "intro": "Our research in 3D generation and reconstruction covers Gaussian Splatting, NeRF, and 3D-aware diffusion, aiming for real-time rendering, continuous level-of-detail control, and semantically consistent 3D scene synthesis.",
        "intro_zh": "探索3D世界的生成式建模与高保真重建。研究方向涵盖 Gaussian Splatting、NeRF、3D-aware diffusion 等技术，用于实现实时渲染、连续细节层次（LOD）控制与语义一致的三维场景生成。",
        "papers": [
            {
                "title": "🧸 G3PT: Unleash the Power of Autoregressive Modeling in 3D Generative Tasks",
                "intro": "The first native 3D generation foundational model based on next-scale autoregression.",
                "intro_zh": "首个基于多尺度自回归的原生 3D 生成基座大模型。",
                "arxiv": {
                    "id": "2409.06322"
                },
                "publish": {
                    "id": "IJCAI 2025",
                    "url": "https://www.ijcai.org/proceedings/2025/262"
                }
            },
            {
                "title": "🏙 Global-Guided Focal Neural Radiance Field for Large-Scale Scene Representation",
                "intro": "GF-NeRF introduces a global-guided two-stage architecture to achieve consistent and high-fidelity large-scale scene rendering without relying on prior scene knowledge.",
                "intro_zh": "GF-NeRF 通过全局引导的双阶段架构，实现无需先验知识的大规模场景一致且高保真渲染。",
                "project": {
                    "id": "GF-NeRF",
                    "url": "https://shaomq2187.github.io/GF-NeRF/"
                },
                "arxiv": {
                    "id": "2403.12839"
                },
                "publish": {
                    "id": "WACV 2025",
                    "url": "https://ieeexplore.ieee.org/abstract/document/10943871"
                }
            },
            {
                "title": "💠 CLoD-GS: Continuous Level-of-Detail Gaussian Splatting for Real-Time Rendering",
                "intro": "CLoD-GS equips 3D Gaussian Splatting with learnable distance-adaptive opacity, enabling smooth, storage-efficient, artifact-free continuous level-of-detail rendering from a single model.",
                "intro_zh": "CLoD-GS 通过引入可学习的距离自适应透明度，为 3D 高斯喷溅表示实现单一模型内平滑、无存储冗余、无跳变伪影的连续细节层次渲染。",
                "arxiv": {
                    "id": "2510.09997"
                },
                "github": {
                    "repo": "amap-cvlab/CLoD-GS"
                }
            }
        ]
    },
    {
        "icon": "🧠",
        "title": "General Deep Learning",
        "title_zh": "通用深度学习",
        "intro": "We focus on general representation learning and model optimization as the foundation for multimodal and cross-domain AI systems. Our research includes Transformer architecture optimization, distributed training, model compression, and preference alignment (DPO, RLHF) to enhance generalization and interpretability.",
        "intro_zh": "关注通用表示学习与模型优化，为多模态、跨任务AI系统提供统一基础。研究方向包括 Transformer架构优化、分布式训练、模型压缩 与 偏好对齐学习（DPO, RLHF），持续提升模型的泛化性与可解释性。",
        "papers": [
            {
                "title": "🎙️ A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
                "intro": "Performance analysis and novel solution exploration for speech recognition under synthetic speech interference.",
                "intro_zh": "合成语音干扰下，语音识别性能分析和新方案探索。",
                "publish": {
                    "id": "ICASSP 2024",
                    "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10446991"
                }
            },
            {
                "title": "Doubly-Fused ViT: Fuse Information from Dual Vision Transformer Streams",
                "intro": "DFvT introduces a doubly-fused Vision Transformer that combines efficient global context modeling with fine-grained spatial detail preservation to achieve high accuracy and efficiency.",
                "intro_zh": "DFvT 提出一种双融合视觉Transformer架构，兼顾全局上下文建模与精细空间细节保留，在保证高效率的同时实现高精度表现。",
                "publish": {
                    "id": "ECCV 2022",
                    "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830723.pdf"
                },
                "github": {
                    "repo": "ginobilinie/DFvT"
                }
            },
            {
                "title": "SCMT: Self-Correction Mean Teacher for Semi-supervised Object Detection",
                "intro": "A self-correction mean teacher architecture that mitigates the impact of noisy pseudo-labels, offering a novel technological breakthrough in the field of semi-supervised object detection.",
                "intro_zh": "一种通过自我校正的教师架构来减少噪声伪标签影响的半监督目标检测新方法。",
                "publish": {
                    "id": "IJCAI 2022",
                    "url": "https://www.ijcai.org/proceedings/2022/0207.pdf"
                }
            },
            {
                "title": "DPOSE: Online Keypoint-CAM Guided Inference for Driver Pose Estimation",
                "intro": "An optimization scheme for a proprietary HPE task in DMS scenarios which involves a pose-wise hard mining strategy for distribution balance and an online keypoint-aligned Grad-CAM loss to constrain activations to semantic regions.",
                "intro_zh": "针对DMS场景下的HPE任务，提出包含困难样本挖掘与在线关键点对齐Grad-CAM损失的优化方案。",
                "publish": {
                    "id": "CVPR Workshop 2023",
                    "url": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Guo_DPOSE_Online_Keypoint-CAM_Guided_Inference_for_Driver_Pose_Estimation_With_CVPRW_2023_paper.pdf"
                }
            }
        ]
    },
    {
        "icon": "",
        "title": "",
        "title_zh": "",
        "intro": "",
        "intro_zh": "",
        "papers": [
            {
                "title": "",
                "intro": "",
                "intro_zh": "",
                "project": {
                    "id": "",
                    "url": ""
                },
                "arxiv": {
                    "id": ""
                },
                "publish": {
                    "id": "",
                    "url": ""
                },
                "github": {
                    "repo": ""
                },
                "hf_model": {
                    "id": ""
                },
                "hf_space": {
                    "id": ""
                },
                "hf_dataset": {
                    "id": ""
                }
            }
        ]
    }
]