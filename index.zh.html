<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>阿里巴巴 高德视觉技术中心</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <nav>
        <div class="logo-area">
            <!-- 修正 Logo 路径 -->
            <img src="profile/assets/amap-cvlab.png" alt="Logo">
            <span>阿里巴巴 高德视觉技术中心</span>
        </div>
        <div class="nav-links">
            <a href="#about">简介</a>
            <a href="#news">动态</a>
            <a href="#tech">技术</a>
            <a href="https://github.com/amap-cvlab">GitHub</a>
            <a href="index.html" class="lang-btn">English</a>
        </div>
    </nav>

    <section class="hero" id="about">
        <h1>阿里巴巴 高德视觉技术中心</h1>
        <div class="hero-content">
            <p>高德视觉技术中心专注于以计算机视觉为核心的前沿研究与创新应用，致力于打造时空互联网领域的核心技术能力。<br>我们立足于现实世界与数字世界的交汇点，以AI驱动真实世界的智能化理解与生成，赋能智慧出行、生活服务与虚拟空间构建。</p>
            <p>作为业界领航者，团队不仅在计算机视觉领域持续深耕，更将计算机视觉及AI技术应用在自主导航、高德打车、生活服务等多元化场景。<br>作为高德地图的核心技术驱动部门，我们的研究方向涵盖从感知到生成、从人本智能到世界建模的全链条技术体系，形成了六大研究领域：</p>
            <ul><li>🗺️ <strong>地图与自动驾驶（Map & Autonomous Driving）：</strong>融合多模态感知与高精地图生成，探索空间语义理解与规则感知下的智能出行。</li><li>🕺🏻 <strong>数字人（Human-Centric AI）：</strong>构建以人为中心的AI系统，让模型理解情感、身份与行为，实现自然的视觉生成与交互。</li><li>🧭 <strong>具身智能（Embodied AI）：</strong>研究智能体在虚拟与真实世界中的感知、规划与行动，推动视觉语言与运动智能的统一。</li><li>🌐 <strong>世界模型（World Modeling）：</strong>构建动态可交互的世界建模框架，让AI具备对环境的理解、预测与生成能力。</li><li>🧊 <strong>3D生成与重建（3D Generation & Reconstruction）：</strong>聚焦三维场景的建模、渲染与生成，实现连续细节控制与真实感表达。</li><li>🧠 <strong>通用深度学习（General Deep Learning）：</strong>探索通用表征、模型优化与多模态对齐，为时空智能提供底层算法支撑。</li></ul>
            <p>高德视觉技术中心始终站在计算机视觉研究与应用的创新高地，是高德空间智能互联网的重要技术实践者。我们相信，<strong>AI对世界的理解能力，将决定未来出行与生活的智能化水平</strong>。</p>
        </div>
    </section>

    <div class="container">
        <section id="news" style="margin-bottom: 4rem;">
            <h2 class="section-title">最新动态</h2>
            <ul class="news-list"><li>🏛 <strong>2026年1月</strong> – 我们非常高兴地宣布，团队共有 <strong>7 篇论文</strong> 被 ICLR 2026 接收，展示了我们在地图与自动驾驶 (<strong><a href="https://wallelwan.github.io/OMA-MAT/" target="_blank">Online Navigation Refinement</a></strong>)、具身智能 (<strong><a href="https://miv-xjtu.github.io/JanusVLN.github.io/" target="_blank">JanusVLN</a></strong>, <strong><a href="https://ce-nav.github.io/" target="_blank">CE-Nav</a></strong>, <strong><a href="https://arxiv.org/abs/2509.25687" target="_blank">OmniNav</a></strong>)、世界模型 (<strong><a href="https://fantasy-amap.github.io/fantasy-world/" target="_blank">FantasyWorld</a></strong>) 以及 3D 生成 (<strong>Sat3DGen</strong>, <strong>CLoD-GS</strong>) 领域的最新突破。</li><li>📢 <strong>2026年01月</strong> – 我们正式开源了 <a href="https://fantasy-amap.github.io/fantasy-vln/" target="_blank"><strong>FantasyVLN</strong></a> 的训练和推理代码及模型权重。</li><li>🏆 <strong>2025年12月</strong> - <a href="https://fantasy-amap.github.io/fantasy-world/" target="_blank"><strong>FantasyWorld</strong></a> 在<strong>斯坦福李飞飞教授团队</strong>发布的 <a href="https://huggingface.co/spaces/Howieeeee/WorldScore_Leaderboard" target="_blank"><strong>WorldScore</strong></a> 评测中斩获<strong>第一名</strong>，验证了我们的技术路线。</li><li>🏛 <strong>2025年11月</strong> – 我们共有 <strong>5 篇论文</strong> 入选 AAAI 2026，包括 1 篇 Oral，涵盖自动驾驶(<a href="https://arxiv.org/abs/2509.22262" target="_blank"><strong>UniMapGen</strong></a> (<em>Oral</em>), <a href="https://miv-xjtu.github.io/PriorDrive/" target="_blank"><strong>PriorDrive</strong></a>, <a href="https://miv-xjtu.github.io/PAMR/" target="_blank"><strong>PAMR</strong></a>)、以及来自我们 <a href="https://fantasy-amap.github.io/" target="_blank"><strong>Fantasy AIGC Family</strong></a> 的视频生成与数字人：<a href="https://fantasy-amap.github.io/fantasy-talking2/" target="_blank"><strong>FantasyTalking2</strong></a>, <a href="https://fantasy-amap.github.io/fantasy-hsi/" target="_blank"><strong>FantasyHSI</strong></a></li><li>🏛 <strong>2025年09月</strong> – 我们的论文 <a href="https://miv-xjtu.github.io/FSDrive.github.io/" target="_blank"><strong>FutureSightDrive</strong></a> 被 NeurIPS 2025 接收，并选为 Spotlight。</li><li>🏛 <strong>2025年07月</strong> – 我们的论文 <a href="https://arxiv.org/pdf/2504.04842" target="_blank"><strong>FantasyTalking</strong></a> 被 ACM MM 2025 接收。</li><li>🏛 <strong>2025年06月</strong> – 我们的论文 <a href="https://arxiv.org/pdf/2507.04822v1" target="_blank"><strong>SeqGrowGraph</strong></a> 被 ICCV 2025 接收。</li><li>📢 <strong>2025年05月</strong> – 我们发布了 <a href="https://miv-xjtu.github.io/FSDrive.github.io/" target="_blank"><strong>FSDrive</strong></a> 的完整项目。</li><li>🏛 <strong>2025年04月</strong> – 我们的论文 <a href="https://arxiv.org/abs/2409.06322" target="_blank"><strong>G3PT</strong></a> 被 IJCAI 2025 接收。</li><li>📢 <strong>2025年04月</strong> – 我们发布了 <a href="https://fantasy-amap.github.io/fantasy-talking/" target="_blank"><strong>FantasyTalking</strong></a>、<a href="https://fantasy-amap.github.io/fantasy-id/" target="_blank"><strong>FantasyID</strong></a> 的推理代码和模型权重。</li></ul>
        </section>

        <section id="tech">
            <h2 class="section-title">公开技术</h2>
            
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">🗺️</div>
                <div class="domain-title">地图与自动驾驶</div>
            </div>
            <div class="domain-intro">融合感知、地图与决策的核心技术，推动高精地图、自动驾驶感知与时空智能的深度融合。团队聚焦于构建下一代 3D 地图引擎、交通规则理解与场景级行为建模，让 AI 在真实城市道路中具备空间理解与可解释决策能力。</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">🛣 Online Navigation Refinement: Achieving Lane-Level Guidance by Associating Standard-Definition and Online Perception Maps</div>
                    <div class="paper-desc">首个在线导航细化基准，提出路径感知Transformer将标准地图与在线感知关联，并统一了全局拓扑与实时几何信息，从而实现了低成本的车道级导航。</div>
                    <div class="badge-container"><a href="https://wallelwan.github.io/OMA-MAT/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICLR 2026</a><a href="https://arxiv.org/abs/2507.07487" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/WallelWan/OMA-MAT" target="_blank" class="badge badge-github" data-github-repo="WallelWan/OMA-MAT"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🚘 FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving</div>
                    <div class="paper-desc">在自动驾驶方向首次提出一种时空思维链的推理方法，提出了视觉生成与理解统一的预训练范式，允许模型可视化地思考，基于当前观察和预测的未来世界进行轨迹规划。</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/FSDrive.github.io/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://neurips.cc/virtual/2025/poster/116" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> NeurIPS 2025 (Spotlight)</a><a href="https://arxiv.org/abs/2505.17685" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/MIV-XJTU/FSDrive" target="_blank" class="badge badge-github" data-github-repo="MIV-XJTU/FSDrive"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🗺 UniMapGen: A Generative Frameworkfor Large-Scale Map Construction from Multi-modal Data</div>
                    <div class="paper-desc">一个生成式统一框架，可以通过自回归方式从多模态输入中生成平滑且拓扑一致的矢量化地图，实现可扩展的、对遮挡具有鲁棒性的大规模城市地图构建，同时无需昂贵的现场数据采集。</div>
                    <div class="badge-container"><a href="https://amap-cvlab.github.io/UniMapGen/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/pdf/2509.22262" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> AAAI 2026 (Oral)</a><a href="https://arxiv.org/abs/2509.22262" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/amap-cvlab/UniMapGen" target="_blank" class="badge badge-github" data-github-repo="amap-cvlab/UniMapGen"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🛣️ PriorDrive: Enhancing Online HD Mapping with Unified Vector Priors</div>
                    <div class="paper-desc">首个能够统一编码并融合多种矢量化先验地图（如SD地图、过期HD地图、历史地图），以增强在线高精地图构建的框架。</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/PriorDrive/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2409.05352" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> AAAI 2026</a><a href="https://arxiv.org/abs/2409.05352" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/MIV-XJTU/PriorDrive" target="_blank" class="badge badge-github" data-github-repo="MIV-XJTU/PriorDrive"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🚥 Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving</div>
                    <div class="paper-desc">本文提出一种融合几何与规则的生成式联合推理范式（PAMR），它以自回归方式统一构建车道向量与持久化交通规则，赋予车辆跨越长时序的场景记忆与规则遵循能力。</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/PAMR/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2509.22756" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/MIV-XJTU/PAMR" target="_blank" class="badge badge-github" data-github-repo="MIV-XJTU/PAMR"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">📑 SeqGrowGraph: Learning Lane Topology as a Chain of Graph Expansions</div>
                    <div class="paper-desc">一种以增量式构建邻接矩阵过程重新阐释车道网学习的生成框架。</div>
                    <div class="badge-container"><a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_SeqGrowGraph_Learning_Lane_Topology_as_a_Chain_of_Graph_Expansions_ICCV_2025_paper.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICCV 2025</a><a href="https://arxiv.org/abs/2507.04822v1" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🚗 Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map</div>
                    <div class="paper-desc">交通标志中的车道级交通规则理解与绑路评测基准及多模态解决方案。</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/MapDR/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign_CVPR_2025_paper.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> CVPR 2025 (Highlight)</a><a href="https://arxiv.org/abs/2410.23780" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">🕺🏻</div>
                <div class="domain-title">数字人</div>
            </div>
            <div class="domain-intro">以生成式AI为核心，探索数字人从“被驱动”到“自主行动”的进化。团队提出 <a href="https://github.com/Fantasy-AMAP" target="_blank">Fantasy AIGC 系列模型</a>，覆盖表情驱动、语音驱动、身份保持与动作生成，实现情感丰富、身份一致、物理合理的高保真数字人视频生成。</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">🗣️ FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</div>
                    <div class="paper-desc">首个基于 Wan 的高保真音频驱动虚拟人系统，通过双阶段音视对齐与可控运动调制，实现动态场景下面部表情、唇动与身体姿态的精准同步。</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-talking/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://dl.acm.org/doi/10.1145/3746027.3755217" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ACM MM 2025</a><a href="https://arxiv.org/abs/2504.04842" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-talking" target="_blank" class="badge badge-github" data-github-repo="Fantasy-AMAP/fantasy-talking"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://huggingface.co/acvlab/FantasyTalking" target="_blank" class="badge badge-huggingface"><i class="fas fa-face-smile"></i> HuggingFace</a><a href="https://modelscope.cn/models/amap_cvlab/FantasyTalking" target="_blank" class="badge badge-modelscope"><i class="fas fa-cube"></i> ModelScope</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🎙️ FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation</div>
                    <div class="paper-desc">一种新颖的“时间步-网络层”自适应多专家偏好优化(TLPO)方法，在口型一致、动作自然、视觉效果三个维度上提升了音频驱动数字人动画的质量。</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-talking2/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://doi.org/10.48550/arXiv.2508.11255" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> AAAI 2026</a><a href="https://arxiv.org/abs/2508.11255v1" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-talking2" target="_blank" class="badge badge-github" ><i class="fab fa-github"></i> Coming Soon</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🗿 FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework</div>
                    <div class="paper-desc">一种基于图结构的多智能体框架，将视频生成与三维世界动态相融合，使数字人具备感知、规划与自主行动的能力，从而在技术层面上成为连接人与世界的统一“感知–行动”推理桥梁。</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-hsi/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://doi.org/10.48550/arXiv.2509.01232" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> AAAI 2026</a><a href="https://arxiv.org/abs/2509.01232" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-hsi" target="_blank" class="badge badge-github" ><i class="fab fa-github"></i> Coming Soon</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🤡 FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers</div>
                    <div class="paper-desc">一种全新的表情驱动视频生成方法，将情绪增强学习与掩码交叉注意力相结合，可在单人或多人肖像场景中生成高质量且富有表现力的动画。</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-portrait/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2507.12956" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-portrait" target="_blank" class="badge badge-github" data-github-repo="Fantasy-AMAP/fantasy-portrait"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🆔 FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation</div>
                    <div class="paper-desc">以3D面部先验、多视角增强以及层感知注入的提升运动场景下的ID保持视频生成框架。</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-id/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2502.13995" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-id" target="_blank" class="badge badge-github" data-github-repo="Fantasy-AMAP/fantasy-id"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://huggingface.co/acvlab/FantasyID" target="_blank" class="badge badge-huggingface"><i class="fas fa-face-smile"></i> HuggingFace</a><a href="https://modelscope.cn/models/amap_cvlab/FantasyID" target="_blank" class="badge badge-modelscope"><i class="fas fa-cube"></i> ModelScope</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">💃🏻 HumanRig: Learning Automatic Rigging for Humanoid Characters in Animation</div>
                    <div class="paper-desc">首个面向3D生成数字人的自动绑骨数据集以及基于变换器的端到端自动绑骨算法。</div>
                    <div class="badge-container"><a href="https://c8241998.github.io/HumanRig/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large_CVPR_2025_paper.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> CVPR 2025 (Highlight)</a><a href="https://arxiv.org/abs/2412.02317" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/c8241998/HumanRig" target="_blank" class="badge badge-github" data-github-repo="c8241998/HumanRig"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://huggingface.co/jellyczd/HumanRig" target="_blank" class="badge badge-huggingface"><i class="fas fa-face-smile"></i> HuggingFace</a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">🧭</div>
                <div class="domain-title">具身智能</div>
            </div>
            <div class="domain-intro">研究智能体在虚拟与物理环境中的感知、思考与行动机制。通过视觉语言模型与强化学习的结合，构建可在三维空间中感知环境、规划目标、执行任务的具身智能体，为机器人与虚拟人提供统一的认知框架。</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">🧠 JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation</div>
                    <div class="paper-desc">首个具备双重隐式记忆的视觉语言导航智能体，解耦视觉语义和空间感知，并分别建模为紧凑的隐式神经表示。</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/JanusVLN.github.io/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICLR 2026</a><a href="https://arxiv.org/abs/2509.22548" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/MIV-XJTU/JanusVLN" target="_blank" class="badge badge-github" data-github-repo="MIV-XJTU/JanusVLN"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://modelscope.cn/models/misstl/JanusVLN_Extra" target="_blank" class="badge badge-modelscope"><i class="fas fa-cube"></i> ModelScope</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation</div>
                    <div class="paper-desc">一个新颖的跨具身实体的局部导航框架，可用作一脑多形、可插拔的快系统。</div>
                    <div class="badge-container"><a href="https://ce-nav.github.io/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICLR 2026</a><a href="https://arxiv.org/abs/2509.23203" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/amap-cvlab/CE-Nav" target="_blank" class="badge badge-github" data-github-repo="amap-cvlab/CE-Nav"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation</div>
                    <div class="paper-desc">OmniNav提出统一的机器人导航框架，以低延迟的连续航点策略与快慢协同规划结合多任务、通用视觉语言数据增强理解能力，在指令目标、物体目标、点目标及前沿探索任务上实现更高精度、泛化与成功率，并获真实部署验证。</div>
                    <div class="badge-container"><a href="" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICLR 2026</a><a href="https://arxiv.org/abs/2509.25687" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/amap-cvlab/OmniNav" target="_blank" class="badge badge-github" data-github-repo="amap-cvlab/OmniNav"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🕵🏻‍♂️ FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-and-Language Navigation</div>
                    <div class="paper-desc">一个统一的多模态链式思维推理框架，通过将世界模型的推演能力内化到 VLN 架构中，基于自然语言指令和视觉观察，实现高效且精确的导航。</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-vln/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2601.13976" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-vln" target="_blank" class="badge badge-github" data-github-repo="Fantasy-AMAP/fantasy-vln"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://huggingface.co/acvlab/FantasyVLN" target="_blank" class="badge badge-huggingface"><i class="fas fa-face-smile"></i> HuggingFace</a><a href="https://modelscope.cn/models/amap_cvlab/FantasyVLN" target="_blank" class="badge badge-modelscope"><i class="fas fa-cube"></i> ModelScope</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA</div>
                    <div class="paper-desc">融合空间结构与动态推理的视觉-语言-动作新范式。</div>
                    <div class="badge-container"><a href="https://arxiv.org/abs/2509.26251" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">🌐</div>
                <div class="domain-title">世界模型</div>
            </div>
            <div class="domain-intro">致力于构建动态、可交互的世界模型，用于理解、预测与生成物理一致的时空过程。通过跨模态数据建模与生成式学习，实现从感知到模拟的闭环，让AI具备理解真实世界的能力。</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">🌏 FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction</div>
                    <div class="paper-desc">一个统一视频先验信息和几何3D的世界模型，能够生成几何一致的、可探索的3D场景。</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-world/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICLR 2026</a><a href="https://arxiv.org/abs/2509.21657" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-world" target="_blank" class="badge badge-github" ><i class="fab fa-github"></i> Coming Soon</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training</div>
                    <div class="paper-desc">一个新颖的以世界模型为虚拟环境的VLA后训练框架。</div>
                    <div class="badge-container"><a href="https://arxiv.org/abs/2509.24948" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">🧊</div>
                <div class="domain-title">3D生成与重建</div>
            </div>
            <div class="domain-intro">探索3D世界的生成式建模与高保真重建。研究方向涵盖 Gaussian Splatting、NeRF、3D-aware diffusion 等技术，用于实现实时渲染、连续细节层次（LOD）控制与语义一致的三维场景生成。</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">🛰 Sat3DGen: Comprehensive Street-Level 3D Scene Generation from Single Satellite Image</div>
                    <div class="paper-desc">一种基于几何优先策略的街景3D前馈式生成框架，无需3D标注，仅用单张遥感图像，通过几何优先约束统一实现高保真三维几何重建与街景视频生成。</div>
                    <div class="badge-container"><a href="" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICLR 2026</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">💠 CLoD-GS: Continuous Level-of-Detail Gaussian Splatting for Real-Time Rendering</div>
                    <div class="paper-desc">CLoD-GS 通过引入可学习的距离自适应透明度，为 3D 高斯喷溅表示实现单一模型内平滑、无存储冗余、无跳变伪影的连续细节层次渲染。</div>
                    <div class="badge-container"><a href="" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICLR 2026</a><a href="https://arxiv.org/abs/2510.09997" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🧸 G3PT: Unleash the Power of Autoregressive Modeling in 3D Generative Tasks</div>
                    <div class="paper-desc">首个基于多尺度自回归的原生 3D 生成基座大模型。</div>
                    <div class="badge-container"><a href="https://www.ijcai.org/proceedings/2025/262" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> IJCAI 2025</a><a href="https://arxiv.org/abs/2409.06322" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🏙 Global-Guided Focal Neural Radiance Field for Large-Scale Scene Representation</div>
                    <div class="paper-desc">GF-NeRF 通过全局引导的双阶段架构，实现无需先验知识的大规模场景一致且高保真渲染。</div>
                    <div class="badge-container"><a href="https://shaomq2187.github.io/GF-NeRF/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://ieeexplore.ieee.org/abstract/document/10943871" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> WACV 2025</a><a href="https://arxiv.org/abs/2403.12839" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">🎨 MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control</div>
                    <div class="paper-desc">基于几何控制的多视角扩散模型，从单张参考图像生成高精度、细节丰富且几何一致的3D纹理及PBR材质。</div>
                    <div class="badge-container"><a href="https://amap-cvlab.github.io/MV-Painter" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2505.12635" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/amap-cvlab/MV-Painter" target="_blank" class="badge badge-github" data-github-repo="amap-cvlab/MV-Painter"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">🧠</div>
                <div class="domain-title">通用深度学习</div>
            </div>
            <div class="domain-intro">关注通用表示学习与模型优化，为多模态、跨任务AI系统提供统一基础。研究方向包括 Transformer架构优化、分布式训练、模型压缩 与 偏好对齐学习（DPO, RLHF），持续提升模型的泛化性与可解释性。</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">🎙️ A Study on the Adverse Impact of Synthetic Speech on Speech Recognition</div>
                    <div class="paper-desc">合成语音干扰下，语音识别性能分析和新方案探索。</div>
                    <div class="badge-container"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10446991" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICASSP 2024</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">Doubly-Fused ViT: Fuse Information from Dual Vision Transformer Streams</div>
                    <div class="paper-desc">DFvT 提出一种双融合视觉Transformer架构，兼顾全局上下文建模与精细空间细节保留，在保证高效率的同时实现高精度表现。</div>
                    <div class="badge-container"><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830723.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ECCV 2022</a><a href="https://github.com/ginobilinie/DFvT" target="_blank" class="badge badge-github" data-github-repo="ginobilinie/DFvT"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">SCMT: Self-Correction Mean Teacher for Semi-supervised Object Detection</div>
                    <div class="paper-desc">一种通过自我校正的教师架构来减少噪声伪标签影响的半监督目标检测新方法。</div>
                    <div class="badge-container"><a href="https://www.ijcai.org/proceedings/2022/0207.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> IJCAI 2022</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">DPOSE: Online Keypoint-CAM Guided Inference for Driver Pose Estimation</div>
                    <div class="paper-desc">针对DMS场景下的HPE任务，提出包含困难样本挖掘与在线关键点对齐Grad-CAM损失的优化方案。</div>
                    <div class="badge-container"><a href="https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Guo_DPOSE_Online_Keypoint-CAM_Guided_Inference_for_Driver_Pose_Estimation_With_CVPRW_2023_paper.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> CVPR Workshop 2023</a></div>
                </div>
            </div></div>
        </section>
    </div>

    <footer>
        <p>© 2026 阿里巴巴 高德视觉技术中心. 保留所有权利。</p>
        <div style="margin-top: 20px;"><a href="https://mapmyvisitors.com/web/1c15e" title="Visit tracker"><img src="https://mapmyvisitors.com/map.png?d=wQjV7K6tiWOnmLQP6EHW6Nts04cNKhPIGb7ZB3X_WcI&cl=ffffff" /></a></div>
    </footer>
    <script src="script.js"></script>
</body>
</html>