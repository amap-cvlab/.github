<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alibaba AMAP CV Lab</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <nav>
        <div class="logo-area">
            <!-- ‰øÆÊ≠£ Logo Ë∑ØÂæÑ -->
            <img src="profile/assets/amap-cvlab.png" alt="Logo">
            <span>Alibaba AMAP CV Lab</span>
        </div>
        <div class="nav-links">
            <a href="#about">About</a>
            <a href="#news">News</a>
            <a href="#tech">Technologies</a>
            <a href="index.zh.html" class="lang-btn">‰∏≠Êñá</a>
        </div>
    </nav>

    <section class="hero" id="about">
        <h1>Alibaba AMAP CV Lab</h1>
        <div class="hero-content">
            <p>The Alibaba AMAP CV Lab focuses on cutting-edge research and innovative applications centered around computer vision technology, dedicated to building the core technological capabilities of the spatiotemporal internet. Positioned at the intersection of the physical and digital worlds, we empower smart mobility, daily life, and virtual spaces through AI-driven understanding and generation.</p>
            <p>As the core technical driving force behind AMAP, our research spans the entire chain from perception to generation, and from human-centric intelligence to world modeling. We are structured into six major research domains:</p>
            <ul><li>üó∫Ô∏è <strong>Map & Autonomous Driving:</strong> Integrating multimodal perception with high-definition map generation to enable spatial semantic understanding and regulation-aware intelligent driving.</li><li>üï∫üèª <strong>Human-Centric AI:</strong> Building AI systems that understand human emotion, identity, and behavior to achieve natural visual generation and interaction.</li><li>üß≠ <strong>Embodied Intelligence:</strong> Studying agents that perceive, plan, and act within both virtual and physical environments, unifying vision, language, and motion intelligence.</li><li>üåê <strong>World Modeling:</strong> Constructing dynamic, interactive models of the world to empower AI with the ability to understand, predict, and generate complex environments.</li><li>üßä <strong>3D Generation & Reconstruction:</strong> Advancing 3D scene modeling, rendering, and generation with continuous level-of-detail control and physically realistic synthesis.</li><li>üß† <strong>General Deep Learning:</strong> Exploring general representation learning, model optimization, and multimodal alignment as foundational algorithms for spatiotemporal intelligence.</li></ul>
            <p>The AMAP CV Lab stands at the forefront of computer vision research and application, serving as a key technological practitioner in Alibaba‚Äôs spatial intelligent internet.<br>We believe that AI‚Äôs ability to understand the world defines the future of intelligent mobility and everyday life.</p>
        </div>
    </section>

    <div class="container">
        <section id="news" style="margin-bottom: 4rem;">
            <h2 class="section-title">Latest News</h2>
            <ul class="news-list"><li>üèõ <strong>Nov, 2025</strong> ‚Äì We are thrilled to announce that <strong>5 papers</strong> from our team have been accepted to AAAI 2026, including 1 Oral, highlighting our latest breakthroughs in autonomous driving (<a href="https://arxiv.org/abs/2509.22262" target="_blank"><strong>UniMapGen</strong></a> (<em>Oral</em>), <a href="" target="_blank"><strong>PriorDrive</strong></a>, <a href="https://arxiv.org/pdf/2509.22756" target="_blank"><strong>Persistent Autoregressive Mapping</strong></a>) and video generation & digital humans from our <a href="https://github.com/Fantasy-AMAP" target="_blank"><strong>Fantasy AIGC Family</strong></a> (<a href="https://fantasy-amap.github.io/fantasy-talking2/" target="_blank"><strong>FantasyTalking2</strong></a>, <a href="https://fantasy-amap.github.io/fantasy-hsi/" target="_blank"><strong>FantasyHSI</strong></a>).</li><li>üèõ <strong>Sep, 2025</strong> ‚Äì Our paper <a href="https://miv-xjtu.github.io/FSDrive.github.io/" target="_blank"><strong>FutureSightDrive</strong></a> is accepted by NeurIPS 2025 (Spotlight).</li><li>üèõ <strong>Jul, 2025</strong> ‚Äì Our paper <a href="https://fantasy-amap.github.io/fantasy-talking/" target="_blank"><strong>FantasyTalking</strong></a> is accepted by ACM MM 2025.</li><li>üèõ <strong>Jun, 2025</strong> ‚Äì Our paper <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_SeqGrowGraph_Learning_Lane_Topology_as_a_Chain_of_Graph_Expansions_ICCV_2025_paper.pdf" target="_blank"><strong>SeqGrowGraph</strong></a> is accepted by ICCV 2025.</li><li>üì¢ <strong>May, 2025</strong> ‚Äì We released the full project of <a href="https://miv-xjtu.github.io/FSDrive.github.io/" target="_blank"><strong>FSDrive</strong></a>.</li><li>üèõ <strong>Apr, 2025</strong> ‚Äì Our paper <a href="https://arxiv.org/abs/2409.06322" target="_blank"><strong>G3PT</strong></a> is accepted by IJCAI 2025.</li><li>üì¢ <strong>Apr, 2025</strong> ‚Äì We released the inference code and model weights of <a href="https://fantasy-amap.github.io/fantasy-talking/" target="_blank"><strong>FantasyTalking</strong></a>, <a href="https://fantasy-amap.github.io/fantasy-id/" target="_blank"><strong>FantasyID</strong></a>.</li></ul>
        </section>

        <section id="tech">
            <h2 class="section-title">Public Technologies</h2>
            
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">üó∫Ô∏è</div>
                <div class="domain-title">Map & Autonomous Driving</div>
            </div>
            <div class="domain-intro">The core of our research lies in integrating perception, mapping, and decision-making for intelligent transportation. We develop next-generation 3D map engines, traffic rule reasoning, and scene-level behavior modeling, enabling AI to understand spatial context and make interpretable decisions in real-world urban environments.</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">üöò FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving</div>
                    <div class="paper-desc">The first VLA for autonomous driving visual reasoning, which proposes spatio-temporal CoT to think visually about trajectory planning and unifies visual generation and understanding with minimal data.</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/FSDrive.github.io/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://neurips.cc/virtual/2025/poster/116" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> NeurIPS 2025 (Spotlight)</a><a href="https://arxiv.org/abs/2505.17685" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/MIV-XJTU/FSDrive" target="_blank" class="badge badge-github" data-github-repo="MIV-XJTU/FSDrive"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üó∫ UniMapGen: A Generative Frameworkfor Large-Scale Map Construction from Multi-modal Data</div>
                    <div class="paper-desc">A generative unified framework that autoregressively generates smooth and topologically consistent vectorized maps from multi-modal inputs, enabling scalable, occlusion-robust city-scale mapping without costly on-site data collection.</div>
                    <div class="badge-container"><a href="https://amap-cvlab.github.io/UniMapGen/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/pdf/2509.22262" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> AAAI 2026 (Oral)</a><a href="https://arxiv.org/abs/2509.22262" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/amap-cvlab/UniMapGen" target="_blank" class="badge badge-github" data-github-repo="amap-cvlab/UniMapGen"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üõ£Ô∏è PriorDrive: Enhancing Online HD Mapping with Unified Vector Priors</div>
                    <div class="paper-desc">This is the first framework that unifies the encoding and integration of diverse vectorized prior maps (such as SD maps, outdated HD maps, and historical maps) to enhance online HD map construction.</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/PriorDrive/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2409.05352" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> AAAI 2026</a><a href="https://arxiv.org/abs/2409.05352" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/MIV-XJTU/PriorDrive" target="_blank" class="badge badge-github" data-github-repo="MIV-XJTU/PriorDrive"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üö• Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving</div>
                    <div class="paper-desc">Pioneering a generative co-reasoning paradigm in autonomous mapping, this work (PAMR) unifies the autoregressive construction of lane geometry and persistent traffic rules, enabling vehicles to build maps with long-term memory and consistent rule awareness across extended sequences.</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/PAMR/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2509.22756" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/MIV-XJTU/PAMR" target="_blank" class="badge badge-github" data-github-repo="MIV-XJTU/PAMR"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üìë SeqGrowGraph: Learning Lane Topology as a Chain of Graph Expansions</div>
                    <div class="paper-desc">A generative framework that reframes lane network learning as a process of incrementally building an adjacency matrix.</div>
                    <div class="badge-container"><a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_SeqGrowGraph_Learning_Lane_Topology_as_a_Chain_of_Graph_Expansions_ICCV_2025_paper.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICCV 2025</a><a href="https://arxiv.org/abs/2507.04822v1" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üöó Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map</div>
                    <div class="paper-desc">Benchmark and multi-modal approach for integrating lane-level traffic sign regulations into vectorized HD maps.</div>
                    <div class="badge-container"><a href="https://amap-cvlab.github.io/DriveByTheRules/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign_CVPR_2025_paper.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> CVPR 2025 (Highlight)</a><a href="https://arxiv.org/abs/2410.23780" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">üï∫üèª</div>
                <div class="domain-title">Human-Centric AI</div>
            </div>
            <div class="domain-intro">Centered on generative AI, our digital human research advances from driven generation to autonomous action. Through the <a href="https://github.com/Fantasy-AMAP" target="_blank">Fantasy AIGC Family</a>, we achieve expressive, identity-consistent, and physically realistic video generation via multimodal diffusion and 3D-aware modeling.</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">üó£Ô∏è FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</div>
                    <div class="paper-desc">The first Wan-based high-fidelity audio-driven avatar system that synchronizes facial expressions, lip motion, and body gestures in dynamic scenes through dual-stage audio-visual alignment and controllable motion modulation.</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-talking/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://dl.acm.org/doi/10.1145/3746027.3755217" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ACM MM 2025</a><a href="https://arxiv.org/abs/2504.04842" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-talking" target="_blank" class="badge badge-github" data-github-repo="Fantasy-AMAP/fantasy-talking"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://huggingface.co/acvlab/FantasyTalking" target="_blank" class="badge badge-huggingface"><i class="fas fa-face-smile"></i> HuggingFace</a><a href="https://modelscope.cn/models/amap_cvlab/FantasyTalking" target="_blank" class="badge badge-modelscope"><i class="fas fa-cube"></i> ModelScope</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üéôÔ∏è FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation</div>
                    <div class="paper-desc">A novel Timestep-Layer Adaptive Multi-Expert Preference Optimization (TLPO) method enhances the quality of audio-driven avatar in three dimensions: lip-sync, motion naturalness, and visual quality.</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-talking2/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://doi.org/10.48550/arXiv.2508.11255" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> AAAI 2026</a><a href="https://arxiv.org/abs/2508.11255v1" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-talking2" target="_blank" class="badge badge-github" ><i class="fab fa-github"></i> Coming Soon</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üóø FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework</div>
                    <div class="paper-desc">A graph-based multi-agent framework that grounds video generation within 3D world dynamics, enabling digital humans to perceive, plan, and act autonomously, thus serving as the technical bridge that links human modeling to world modeling through unified perception‚Äìaction reasoning.</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-hsi/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://doi.org/10.48550/arXiv.2509.01232" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> AAAI 2026</a><a href="https://arxiv.org/abs/2509.01232" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-hsi" target="_blank" class="badge badge-github" ><i class="fab fa-github"></i> Coming Soon</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">ü§° FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers</div>
                    <div class="paper-desc">A novel expression-driven video-generation method that pairs emotion-enhanced learning with masked cross-attention, enabling the creation of high-quality, richly expressive animations for both single and multi-portrait scenarios.</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-portrait/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2507.12956" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-portrait" target="_blank" class="badge badge-github" data-github-repo="Fantasy-AMAP/fantasy-portrait"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üÜî FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation</div>
                    <div class="paper-desc">A tuning-free text-to-video model that leverages 3D facial priors, multi-view augmentation, and layer-aware guidance injection to deliver dynamic, identity-preserving video generation.</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-id/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2502.13995" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-id" target="_blank" class="badge badge-github" data-github-repo="Fantasy-AMAP/fantasy-id"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://huggingface.co/acvlab/FantasyID" target="_blank" class="badge badge-huggingface"><i class="fas fa-face-smile"></i> HuggingFace</a><a href="https://modelscope.cn/models/amap_cvlab/FantasyID" target="_blank" class="badge badge-modelscope"><i class="fas fa-cube"></i> ModelScope</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üíÉüèª HumanRig: Learning Automatic Rigging for Humanoid Characters in Animation</div>
                    <div class="paper-desc">The first dataset for automatic rigging of 3D generated digital humans and a transformer-based end-to-end automatic rigging algorithm.</div>
                    <div class="badge-container"><a href="https://c8241998.github.io/HumanRig/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large_CVPR_2025_paper.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> CVPR 2025 (Highlight)</a><a href="https://arxiv.org/abs/2412.02317" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/c8241998/HumanRig" target="_blank" class="badge badge-github" data-github-repo="c8241998/HumanRig"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://huggingface.co/jellyczd/HumanRig" target="_blank" class="badge badge-huggingface"><i class="fas fa-face-smile"></i> HuggingFace</a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">üß≠</div>
                <div class="domain-title">Embodied AI</div>
            </div>
            <div class="domain-intro">We study perception, reasoning, and action of intelligent agents in both virtual and physical environments. By integrating vision-language models and reinforcement learning, we build embodied agents capable of environmental perception, goal planning, and task execution, forming a unified cognitive foundation for robots and digital humans.</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation</div>
                    <div class="paper-desc">A novel cross-embodiment local navigation framework, which can serve as a "one brain, multiple forms", plug-and-play fast system.</div>
                    <div class="badge-container"><a href="https://ce-nav.github.io/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2509.23203" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/amap-cvlab/CE-Nav" target="_blank" class="badge badge-github" data-github-repo="amap-cvlab/CE-Nav"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation</div>
                    <div class="paper-desc">OmniNav is a unified embodied navigation framework that combines a lightweight, real-time (up to 5 Hz) continuous waypoint policy with a fast‚Äìslow planning architecture and large-scale vision-language multi-task training to robustly handle instruction-, object-, and point-goal navigation and frontier exploration, achieving state-of-the-art performance and real-world validation.</div>
                    <div class="badge-container"><a href="https://arxiv.org/abs/2509.25687" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/amap-cvlab/OmniNav" target="_blank" class="badge badge-github" ><i class="fab fa-github"></i> Coming Soon</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üß† JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation</div>
                    <div class="paper-desc">The first visual-language navigation agent with dual implicit memory decouples visual semantics and spatial perception and models them respectively as compact implicit neural representations.</div>
                    <div class="badge-container"><a href="https://miv-xjtu.github.io/JanusVLN.github.io/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2509.22548" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/MIV-XJTU/JanusVLN" target="_blank" class="badge badge-github" data-github-repo="MIV-XJTU/JanusVLN"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a><a href="https://modelscope.cn/models/misstl/JanusVLN_Extra" target="_blank" class="badge badge-modelscope"><i class="fas fa-cube"></i> ModelScope</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA</div>
                    <div class="paper-desc">A Robust Vision-Language-Action Framework with Structural Perception and Explicit Dynamics Reasoning.</div>
                    <div class="badge-container"><a href="https://arxiv.org/abs/2509.26251" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">üåê</div>
                <div class="domain-title">World Modeling</div>
            </div>
            <div class="domain-intro">We aim to construct dynamic, interactive world models for understanding, predicting, and generating physically consistent spatiotemporal phenomena. By leveraging multimodal modeling and generative learning, our research enables a perception-to-simulation loop that empowers AI to comprehend and recreate the real world.</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">üåè FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction</div>
                    <div class="paper-desc">A unified world model integrating video priors and geometric grounding for synthesizing explorable and geometrically consistent 3D scenes.</div>
                    <div class="badge-container"><a href="https://fantasy-amap.github.io/fantasy-world/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://arxiv.org/abs/2509.21657" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/Fantasy-AMAP/fantasy-world" target="_blank" class="badge badge-github" ><i class="fab fa-github"></i> Coming Soon</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training</div>
                    <div class="paper-desc">A novel framework leveraging world model as a virtual environment for VLA post training.</div>
                    <div class="badge-container"><a href="https://arxiv.org/abs/2509.24948" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">üßä</div>
                <div class="domain-title">3D Generation & Reconstruction</div>
            </div>
            <div class="domain-intro">Our research in 3D generation and reconstruction covers Gaussian Splatting, NeRF, and 3D-aware diffusion, aiming for real-time rendering, continuous level-of-detail control, and semantically consistent 3D scene synthesis.</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">üß∏ G3PT: Unleash the Power of Autoregressive Modeling in 3D Generative Tasks</div>
                    <div class="paper-desc">The first native 3D generation foundational model based on next-scale autoregression.</div>
                    <div class="badge-container"><a href="https://www.ijcai.org/proceedings/2025/262" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> IJCAI 2025</a><a href="https://arxiv.org/abs/2409.06322" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üèô Global-Guided Focal Neural Radiance Field for Large-Scale Scene Representation</div>
                    <div class="paper-desc">GF-NeRF introduces a global-guided two-stage architecture to achieve consistent and high-fidelity large-scale scene rendering without relying on prior scene knowledge.</div>
                    <div class="badge-container"><a href="https://shaomq2187.github.io/GF-NeRF/" target="_blank" class="badge badge-project"><i class="fas fa-globe"></i> Project</a><a href="https://ieeexplore.ieee.org/abstract/document/10943871" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> WACV 2025</a><a href="https://arxiv.org/abs/2403.12839" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">üí† CLoD-GS: Continuous Level-of-Detail Gaussian Splatting for Real-Time Rendering</div>
                    <div class="paper-desc">CLoD-GS equips 3D Gaussian Splatting with learnable distance-adaptive opacity, enabling smooth, storage-efficient, artifact-free continuous level-of-detail rendering from a single model.</div>
                    <div class="badge-container"><a href="https://arxiv.org/abs/2510.09997" target="_blank" class="badge badge-arxiv"><i class="fas fa-file-pdf"></i> arXiv</a><a href="https://github.com/amap-cvlab/CLoD-GS" target="_blank" class="badge badge-github" data-github-repo="amap-cvlab/CLoD-GS"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            </div></div>
        <div class="domain-block">
            <div class="domain-header">
                <div class="domain-icon">üß†</div>
                <div class="domain-title">General Deep Learning</div>
            </div>
            <div class="domain-intro">We focus on general representation learning and model optimization as the foundation for multimodal and cross-domain AI systems. Our research includes Transformer architecture optimization, distributed training, model compression, and preference alignment (DPO, RLHF) to enhance generalization and interpretability.</div>
            <div class="papers-grid">
        
                <div class="paper-card">
                    <div class="paper-title">üéôÔ∏è A Study on the Adverse Impact of Synthetic Speech on Speech Recognition</div>
                    <div class="paper-desc">Performance analysis and novel solution exploration for speech recognition under synthetic speech interference.</div>
                    <div class="badge-container"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10446991" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ICASSP 2024</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">Doubly-Fused ViT: Fuse Information from Dual Vision Transformer Streams</div>
                    <div class="paper-desc">DFvT introduces a doubly-fused Vision Transformer that combines efficient global context modeling with fine-grained spatial detail preservation to achieve high accuracy and efficiency.</div>
                    <div class="badge-container"><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830723.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> ECCV 2022</a><a href="https://github.com/ginobilinie/DFvT" target="_blank" class="badge badge-github" data-github-repo="ginobilinie/DFvT"><i class="fab fa-github"></i> Code<span class="star-suffix" style="display:none"> (<span class="star-count"></span> <i class="fas fa-star"></i>)</span></a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">SCMT: Self-Correction Mean Teacher for Semi-supervised Object Detection</div>
                    <div class="paper-desc">A self-correction mean teacher architecture that mitigates the impact of noisy pseudo-labels, offering a novel technological breakthrough in the field of semi-supervised object detection.</div>
                    <div class="badge-container"><a href="https://www.ijcai.org/proceedings/2022/0207.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> IJCAI 2022</a></div>
                </div>
            
                <div class="paper-card">
                    <div class="paper-title">DPOSE: Online Keypoint-CAM Guided Inference for Driver Pose Estimation</div>
                    <div class="paper-desc">An optimization scheme for a proprietary HPE task in DMS scenarios which involves a pose-wise hard mining strategy for distribution balance and an online keypoint-aligned Grad-CAM loss to constrain activations to semantic regions.</div>
                    <div class="badge-container"><a href="https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Guo_DPOSE_Online_Keypoint-CAM_Guided_Inference_for_Driver_Pose_Estimation_With_CVPRW_2023_paper.pdf" target="_blank" class="badge badge-publish"><i class="fas fa-landmark"></i> CVPR Workshop 2023</a></div>
                </div>
            </div></div>
        </section>
    </div>

    <footer>
        <p>¬© 2025 Alibaba AMAP CV Lab. All Rights Reserved.</p>
        <div style="margin-top: 20px;"><a href="https://mapmyvisitors.com/web/1c15e" title="Visit tracker"><img src="https://mapmyvisitors.com/map.png?d=wQjV7K6tiWOnmLQP6EHW6Nts04cNKhPIGb7ZB3X_WcI&cl=ffffff" /></a></div>
    </footer>
    <script src="script.js"></script>
</body>
</html>